// @input Asset.Texture cameraTexture {"hint":"Drag the Device Camera Texture here"}
// @input Component.Text emotionText {"hint":"Text component to display emotion"}
// @input float analysisInterval = 3.0 {"hint":"Seconds between emotion analyses"}
// @input string rekaApiKey {"hint":"Your REKA API key"}
// @input bool enableDebugMode = true

const Internet = require("LensStudio:InternetModule");

let isProcessing = false;
let lastAnalysisTime = 0;
let currentEmotion = "Neutral";
let emotionHistory = [];
const MAX_HISTORY = 5;

// Emotion emoji mapping
const EMOTION_EMOJIS = {
    "Happy": "üòä",
    "Sad": "üò¢",
    "Angry": "üò†",
    "Surprised": "üò≤",
    "Fearful": "üò®",
    "Disgusted": "ü§¢",
    "Neutral": "üòê"
};

// Emotion color mapping
const EMOTION_COLORS = {
    "Happy": new vec4(0.2, 1.0, 0.2, 1.0),      // Green
    "Sad": new vec4(0.3, 0.3, 1.0, 1.0),        // Blue
    "Angry": new vec4(1.0, 0.2, 0.2, 1.0),      // Red
    "Surprised": new vec4(1.0, 0.8, 0.2, 1.0),  // Yellow
    "Fearful": new vec4(0.6, 0.2, 0.8, 1.0),    // Purple
    "Disgusted": new vec4(0.5, 0.8, 0.2, 1.0),  // Yellow-Green
    "Neutral": new vec4(1.0, 1.0, 1.0, 1.0)     // White
};

script.createEvent("OnStartEvent").bind(() => {
    if (!script.cameraTexture) {
        print("‚ùó ERROR: Camera texture not set!");
        return;
    }
    
    if (!script.rekaApiKey || script.rekaApiKey === "") {
        print("‚ùó ERROR: REKA API key not set!");
        return;
    }
    
    if (!script.emotionText) {
        print("‚ö†Ô∏è WARNING: Emotion text component not set");
    } else {
        updateEmotionDisplay("Neutral");
    }
    
    if (script.enableDebugMode) {
        print("‚úÖ RekaEmotionAnalyzer initialized");
    }
    
    // Start the analysis loop
    scheduleNextAnalysis();
});

function scheduleNextAnalysis() {
    if (script.enableDebugMode) {
        print("‚è∞ [Loop] Scheduling next analysis in " + script.analysisInterval + " seconds...");
    }
    
    const delayedEvent = script.createEvent("DelayedCallbackEvent");
    delayedEvent.bind(() => {
        if (script.enableDebugMode) {
            print("üîÑ [Loop] Timer fired! Processing: " + isProcessing);
        }
        
        if (!isProcessing) {
            analyzeEmotion();
        } else {
            print("‚è∏Ô∏è [Loop] Skipping analysis - still processing previous request");
        }
        scheduleNextAnalysis(); // Continue the loop
    });
    delayedEvent.reset(script.analysisInterval);
}

async function analyzeEmotion() {
    if (!script.cameraTexture || isProcessing) {
        if (script.enableDebugMode) {
            print("‚ö†Ô∏è [Analysis] Skipped - No texture or already processing");
        }
        return;
    }
    
    isProcessing = true;
    lastAnalysisTime = getTime();
    
    print("üîç [Analysis] Starting emotion analysis... (Time: " + lastAnalysisTime.toFixed(2) + ")");
    
    try {
        // Validate camera texture before encoding
        if (!script.cameraTexture) {
            print("‚ùó [Analysis] Camera texture is null or undefined!");
            isProcessing = false;
            return;
        }
        
        print("üì∏ [Analysis] Encoding camera texture to base64...");
        
        // Convert camera texture to base64
        const base64Image = await encodeTextureToBase64(script.cameraTexture);
        
        if (base64Image && base64Image.length > 0) {
            print("‚úÖ [Analysis] Texture encoded! Size: " + base64Image.length + " chars");
            await sendToRekaAPI(base64Image);
        } else {
            print("‚ùó [Analysis] Failed to encode camera texture - result was empty or undefined");
        }
    } catch (error) {
        print("‚ùó [Analysis] ERROR analyzing emotion: " + (error ? error.toString() : "Unknown error"));
        print("‚ùó [Analysis] Error type: " + (typeof error));
        print("‚ùó [Analysis] Error message: " + (error?.message || "No message"));
    } finally {
        print("‚úîÔ∏è [Analysis] Complete - Setting isProcessing to false");
        isProcessing = false;
    }
}

function encodeTextureToBase64(texture) {
    return new Promise((resolve, reject) => {
        try {
            if (!texture) {
                reject(new Error("Texture is null or undefined"));
                return;
            }
            
            Base64.encodeTextureAsync(
                texture,
                function(result) {
                    if (result && result.length > 0) {
                        resolve(result);
                    } else {
                        reject(new Error("Base64 encoding returned empty result"));
                    }
                },
                function(err) {
                    reject(new Error("Base64 encoding failed: " + (err || "Unknown error")));
                },
                CompressionQuality.LowQuality,
                EncodingType.Jpg
            );
        } catch (e) {
            reject(new Error("Exception in encodeTextureToBase64: " + e));
        }
    });
}

async function sendToRekaAPI(base64Image) {
    print("üåê [API] Preparing REKA API request...");
    
    const requestPayload = {
        messages: [
            {
                role: "user",
                content: [
                    {
                        type: "image_url",
                        image_url: `data:image/jpeg;base64,${base64Image}`
                    },
                    {
                        type: "text",
                        text: "Analyze the human face visible in this image and provide a detailed emotional analysis including: primary emotion (choose from Happy, Sad, Angry, Surprised, Fearful, Disgusted, Neutral), confidence (0‚Äì100%), secondary emotion with confidence, key facial indicators, emotional intensity (0.0‚Äì1.0), valence (Positive, Neutral, Negative), and arousal level (Low, Medium, High). Respond strictly in JSON format with keys: primary_emotion, confidence, secondary_emotion, secondary_confidence, facial_indicators, intensity, valence, arousal."
                    }
                ]
            }
        ],
        model: "reka-core",
        temperature: 0.3,
        max_tokens: 10
    };
    
    print("üì§ [API] Sending request to REKA API...");
    
    try {
        const response = await Internet.fetch(new Request(
            "https://api.reka.ai/v1/chat",
            {
                method: "POST",
                headers: {
                    "Content-Type": "application/json",
                    "Authorization": `Bearer ${script.rekaApiKey}`
                },
                body: JSON.stringify(requestPayload)
            }
        ));
        
        print("üì• [API] Received response - Status: " + response.status);
        
        if (response.status === 200) {
            const data = await response.json();
            print("‚úÖ [API] Response parsed successfully");

            // Robust extraction of emotion text from various possible response shapes
            let emotionText = "";

            // REKA response shape: { responses: [ { message: { content: "... Neutral." } } ] }
            if (!emotionText && Array.isArray(data?.responses) && data.responses.length > 0) {
                var firstResp = data.responses[0];
                if (firstResp && firstResp.message && typeof firstResp.message.content === 'string') {
                    var raw = firstResp.message.content.trim();
                    // Take the last word, strip punctuation
                    var parts = raw.split(/\s+/);
                    var lastWord = parts.length > 0 ? parts[parts.length - 1] : "";
                    lastWord = lastWord.replace(/[^A-Za-z]+$/g, "");
                    emotionText = lastWord;
                    print("üéØ [API] REKA responses[0].message.content last word: '" + emotionText + "'");
                }
            }

            // OpenAI-like shape
            if (data && data.choices && data.choices.length > 0 &&
                data.choices[0] && data.choices[0].message && typeof data.choices[0].message.content === 'string') {
                emotionText = data.choices[0].message.content.trim();
            }

            // Claude/Anthropic-like shape: content is an array of blocks with {type, text}
            if (!emotionText && Array.isArray(data?.content) && data.content.length > 0) {
                const textBlock = data.content.find(function (c) { return typeof c?.text === 'string' && c.text.length > 0; });
                if (textBlock) { emotionText = textBlock.text.trim(); }
            }

            // Generic fallbacks
            if (!emotionText && typeof data?.text === 'string') { emotionText = data.text.trim(); }
            if (!emotionText && typeof data?.output === 'string') { emotionText = data.output.trim(); }
            if (!emotionText && typeof data?.message === 'string') { emotionText = data.message.trim(); }
            if (!emotionText && typeof data?.result === 'string') { emotionText = data.result.trim(); }

            if (emotionText) {
                print("üéØ [API] Raw emotion text: '" + emotionText + "'");

                const emotion = parseEmotion(emotionText);
                print("üîç [Parse] Parsed emotion: " + emotion);

                // Add to history
                emotionHistory.push(emotion);
                if (emotionHistory.length > MAX_HISTORY) {
                    emotionHistory.shift();
                }
                print("üìä [History] Emotion added. History size: " + emotionHistory.length);

                // Get smoothed emotion (most common in recent history)
                const smoothedEmotion = getSmoothedEmotion();
                print("üìà [Smooth] Smoothed emotion: " + smoothedEmotion + " (previous: " + currentEmotion + ")");

                if (smoothedEmotion !== currentEmotion) {
                    currentEmotion = smoothedEmotion;
                    updateEmotionDisplay(smoothedEmotion);
                    print("üòä [Result] Emotion CHANGED to: " + smoothedEmotion);
                } else {
                    print("‚û°Ô∏è [Result] Emotion UNCHANGED: " + currentEmotion);
                }
            } else {
                // Debug: print a compact snapshot of the response for troubleshooting
                var keys = Object.keys(data || {});
                var preview = JSON.stringify(data).slice(0, 500);
                print("‚ùó [API] Unexpected REKA response shape. Keys: " + keys.join(","));
                print("üìù [API] Body preview: " + preview + (preview.length === 500 ? "..." : ""));
            }
        } else {
            print("‚ùó [API] REKA API error - Status: " + response.status);
            const errorText = await response.text();
            print("‚ùó [API] Error details: " + errorText);
        }
    } catch (error) {
        print("‚ùó [API] REKA request FAILED with exception: " + error);
    }
}

function parseEmotion(emotionText) {
    const validEmotions = ["Happy", "Sad", "Angry", "Surprised", "Fearful", "Disgusted", "Neutral"];
    const normalized = emotionText.trim();
    
    for (let i = 0; i < validEmotions.length; i++) {
        const emotion = validEmotions[i];
        if (normalized.toLowerCase().indexOf(emotion.toLowerCase()) !== -1) {
            return emotion;
        }
    }
    
    return "Neutral";
}

function getSmoothedEmotion() {
    if (emotionHistory.length === 0) {
        return "Neutral";
    }
    
    // Count occurrences
    const counts = {};
    for (let i = 0; i < emotionHistory.length; i++) {
        const emotion = emotionHistory[i];
        counts[emotion] = (counts[emotion] || 0) + 1;
    }
    
    // Find most common
    let maxCount = 0;
    let mostCommon = "Neutral";
    for (const emotion in counts) {
        if (counts[emotion] > maxCount) {
            maxCount = counts[emotion];
            mostCommon = emotion;
        }
    }
    
    return mostCommon;
}

function updateEmotionDisplay(emotion) {
    if (!script.emotionText) {
        return;
    }
    
    const emoji = EMOTION_EMOJIS[emotion] || "üòê";
    const displayText = emoji + " " + emotion;
    
    script.emotionText.text = displayText;
    
    // Set color based on emotion
    const color = EMOTION_COLORS[emotion] || new vec4(1, 1, 1, 1);
    script.emotionText.textFill.color = color;
}

// Public API for other scripts
script.api.getCurrentEmotion = function() {
    return currentEmotion;
};

script.api.getEmotionWithConfidence = function() {
    let count = 0;
    for (let i = 0; i < emotionHistory.length; i++) {
        if (emotionHistory[i] === currentEmotion) {
            count++;
        }
    }
    
    const confidence = emotionHistory.length > 0 ? count / emotionHistory.length : 0;
    
    return {
        emotion: currentEmotion,
        confidence: confidence
    };
};

script.api.forceAnalysis = function() {
    if (!isProcessing) {
        analyzeEmotion();
    }
};